---
layout: post
title: 自然语言处理（一）-TF.IDF
category: 自然语言处理
date: 2014-12-24
math: true
---

<!-- more -->

TF-IDF是一种对文本中词语的重要性进行加权的算法，一般来说，一个词语的重要性随着它的文章中出现的次数成正比增大，但同时会随着它在语料库中出现的频率成反比下降。

1. TF（Term Frequency）
    TF（Term Frequency）：词项频率
    假设文档集中有N篇文档，f(i,j)为词项i在文档j中出现的频率（次数），于是，词项i在文档j中的词项频率可以定义为：
    $$TF(i,j)=\frac{f(i,j)}{max(f(k,j))} $$     也是就词项i在文档j中归一化的结果，其中归一化通过f(i,j)除以同一文档中出现次数最多的词项的频率来算，因此，TF值都是小于或等于1的。

2.IDF（Inverse Document Frequency）
    IDF（Inverse Document Frequency）：逆文档频率
    同样假设文档集中有N篇文档， 如果词项i在n篇文档中出现，那么IDF可以定义为：
    $$ IDF(i)= \log (\frac{N}{n}) $$
    但是，当词项i没有在任何文档中出现时，上面的式子将会出现分母为零的情况，所以通常将IDF定义为： $$ IDF(i)= \log (\frac{N}{n+1}) $$

3.TF.IDF
    TF.IDF表示词项频率乘以逆文档频率。
    基于TF和IDF的定义，词项i在文档j中的得分可以定义为TF(i,j)*IDF（i）也就是：$$w(i,j)={\frac{f(i,j)}{max(f(k,j))}} * \log (\frac{N}{n+1})$$
    这样一来就兼顾了“一个词语的重要性随着它的文章中出现的次数成正比增大”和“同时会随着它在语料库中出现的频率成反比下降”这两种情况。
    因此在一篇文档中TF * IDF得分高的词项往往都是代表这篇文档主题的最佳词项。

4.TF.IDF算法的不足
    TF.IDF算法的前提假设是“一个词语的重要性随着它的文章中出现的次数成正比增大，但同时会随着它在语料库中出现的频率成反比下降”。但是实际情况可能并不都是这样。
    例如在两篇球类（假如是篮球和足球）新闻的报道中，足球新闻中出现了“犯规”和“帽子戏法”，篮球新闻中出现了“犯规”和“盖帽”。现在假如利用TF.IDF来区分这两篇新闻属于
    什么球类新闻的话，恐怕难易做到（假设其他参数一样），因为TF.IDF中只考虑了词语的出现次数，而实际情况是，有些词语即便只出现一次，就能很直观的反映文章的类别。














