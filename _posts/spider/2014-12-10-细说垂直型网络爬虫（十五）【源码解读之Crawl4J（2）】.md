---
layout: post
title: 细说垂直型网络爬虫（十五）【源码解读之Crawl4J（2）】
category: 细说垂直型网络爬虫
date: 2014-12-10

---

##细说垂直型网络爬虫（十五）【源码解读之Crawl4J（2）】

标签： 网络爬虫 开源爬虫源码解读 Crawl4J

###1.Crawl4J简介
Crawl4J是一个开源的Java爬虫程序，总共才三十多个类，比较简单，非常适合爬虫入门的学习。

<!-- more -->

###2.edu.uci.ics.crawler4j.crawler包
edu.uci.ics.crawler4j.crawler包主要负责爬虫的配置和控制，包括的类有：Configurable、CrawlConfig、CrawlController、Page和WebCrawler
先看看**CrawlConfig**类，了解Crawl4J有哪些基本的配置，Crawl4J使用berkeley db来存储抓取的临时数据，如抓取过程中的URL,这样依赖
就可以进行断点抓取。
{% highlight java %}
	/**
	 * The folder which will be used by crawler for storing the intermediate
	 * crawl data. The content of this folder should not be modified manually.
	 * 保存爬取数据的目录
	 *
	 */
	private String crawlStorageFolder;

	/**
	 * If this feature is enabled, you would be able to resume a previously
	 * stopped/crashed crawl. However, it makes crawling slightly slower
	 * 如果为true，在构造时会打开对应env中的database(PendingURLsDB)，获取上一次遗留的未处理的urls
	 */
	private boolean resumableCrawling = false;

	/**
	 * Maximum depth of crawling For unlimited depth this parameter should be
	 * set to -1
	 * 爬取深度
	 */
	private int maxDepthOfCrawling = -1;

	/**
	 * Maximum number of pages to fetch For unlimited number of pages, this
	 * parameter should be set to -1
	 * 最大抓取数
	 *
	 */
	private int maxPagesToFetch = -1;

	/**
	 * user-agent string that is used for representing your crawler to web
	 * servers. See http://en.wikipedia.org/wiki/User_agent for more details
	 * user-agent
	 *
	 */
	private String userAgentString = "crawler4j (http://code.google.com/p/crawler4j/)";

	/**
	 * Politeness delay in milliseconds (delay between sending two requests to
	 * the same host).
	 * 连续发起两次同样的请求的时间间隔 （单位毫秒）
	 *
	 */
	private int politenessDelay = 200;

	/**
	 * Should we also crawl https pages?
	 * 是否抓取Https 加密的网站
	 *
	 */
	private boolean includeHttpsPages = false;

	/**
	 * Should we fetch binary content such as images, audio, ...?
	 * 是否抓取多媒体数据
	 *
	 */
	private boolean includeBinaryContentInCrawling = false;

	/**
	 * Maximum Connections per host
	 * 每个host的最大连接数
	 *
	 */
	private int maxConnectionsPerHost = 100;

	/**
	 * Maximum total connections
	 * 最大连接数
	 *
	 */
	private int maxTotalConnections = 100;

	/**
	 * Socket timeout in milliseconds
	 * Socket超时时间
	 */
	private int socketTimeout = 20000;

	/**
	 * Connection timeout in milliseconds
	 * 连接超时时间设置
	 *
	 */
	private int connectionTimeout = 30000;

	/**
	 * Max number of outgoing links which are processed from a page
	 * 从每个页面中获取的最大输出链接数
	 *
	 */
	private int maxOutgoingLinksToFollow = 5000;

	/**
	 * Max allowed size of a page. Pages larger than this size will not be
	 * fetched.
	 * 页面最大允许大小，超过这个值的页面将不会被保存
	 *
	 */
	private int maxDownloadSize = 1048576;

	/**
	 * Should we follow redirects?
	 *
	 * 是否抓取重定向的网页
	 */
	private boolean followRedirects = true;

	/**
	 * If crawler should run behind a proxy, this parameter can be used for
	 * specifying the proxy host.
	 * 代理服务器地址
	 *
	 */
	private String proxyHost = null;

	/**
	 * If crawler should run behind a proxy, this parameter can be used for
	 * specifying the proxy port.
	 *
	 * 代理服务器接口
	 */
	private int proxyPort = 80;

	/**
	 * If crawler should run behind a proxy and user/pass is needed for
	 * authentication in proxy, this parameter can be used for specifying the
	 * username.
	 * 代理名
	 *
	 */
	private String proxyUsername = null;

	/**
	 * If crawler should run behind a proxy and user/pass is needed for
	 * authentication in proxy, this parameter can be used for specifying the
	 * password.
	 *
	 * 代理密码
	 */
	private String proxyPassword = null;
{% endhighlight %}

**CrawlController**类：
主要属性有：自定义的数据对象、本地历史抓取数据、爬虫是否抓取完成和是否立即停止抓取：

{% highlight java %}
/**
	 * The 'customData' object can be used for passing custom crawl-related
	 * configurations to different components of the crawler.
	 * 自定义数据对象
	 */
	protected Object customData;

	/**
	 * Once the crawling session finishes the controller collects the local data
	 * of the crawler threads and stores them in this List.
	 * 本地历史抓取数据
	 */
	protected List<Object> crawlersLocalData = new ArrayList<Object>();

	/**
	 * Is the crawling of this session finished?
	 * 爬虫是否抓取完成
	 */
	protected boolean finished;

	/**
	 * Is the crawling session set to 'shutdown'. Crawler threads monitor this
	 * flag and when it is set they will no longer process new pages.
	 * 是否立即停止抓取
	 */
	protected boolean shuttingDown;
{% endhighlight %}

主要的方法有：构造方法CrawlController、开启爬虫的方法start、添加种子URL的方法addSeed。















