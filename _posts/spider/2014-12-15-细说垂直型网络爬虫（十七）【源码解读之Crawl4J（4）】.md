---
layout: post
title: 细说垂直型网络爬虫（十七）【源码解读之Crawl4J（4）】
category: 细说垂直型网络爬虫
date: 2014-12-15

---

##细说垂直型网络爬虫（十七）【源码解读之Crawl4J（4）】

标签： 网络爬虫 开源爬虫源码解读 Crawl4J

###1.Crawl4J简介
Crawl4J是一个开源的Java爬虫程序，总共才三十多个类，比较简单，非常适合爬虫入门的学习。
（官方地址：https://code.google.com/p/crawler4j/）

<!-- more -->

###2.edu.uci.ics.crawler4j.frontier包
Counters：计数器
DocIDServer：URL的ID生成器
Frontier：URL调度器
InProcessPagesDB：尚未处理（正在处理）的URL队列
WorkQueues：URL队列
WebURLTupleBinding：自定义的绑定器（转换器）

![开源爬虫Crawl4J的Crawler包](/res/img/blogimg/2014121104-frontier.png)

Counters计数器主要记录已经抓取过了的URL数量和即将抓取的URL数量
DocIDServer可以根据URL获取到对应的ID，也可以根据URL生成新的ID
WebURLTupleBinding主要是将WebURL对象转化为berkeley db的DatabaseEntry以及将DatabaseEntry转化为WebURL
WorkQueues是URL队列，可以获取指定数量的URL，可以删除指定数量的URL
InProcessPagesDB继承自WorkQueues，是即将被抓取的URL，保存从URL队列中取出来即将抓取的URL，当完成抓取后会将对应的URL移除
Frontier是这个包中的核心类

（请注意：下面我将无耻的贴很多代码）
Frontier的主要代码：
{% highlight java %}
	//URL队列
	protected WorkQueues workQueues;
	//正在处理的URL队列
	protected InProcessPagesDB inProcessPages;

	protected final Object mutex = new Object();
	protected final Object waitingList = new Object();

	protected boolean isFinished = false;

	protected long scheduledPages;

	protected DocIDServer docIdServer;

	protected Counters counters;

	/**
	 * 初始化
	 * @param env
	 * @param config
	 * @param docIdServer
	 */
	public Frontier(Environment env, CrawlConfig config, DocIDServer docIdServer) {
		super(config);
		this.counters = new Counters(env, config);
		this.docIdServer = docIdServer;
		try {
			workQueues = new WorkQueues(env, "PendingURLsDB", config.isResumableCrawling());
			if (config.isResumableCrawling()) {
				scheduledPages = counters.getValue(ReservedCounterNames.SCHEDULED_PAGES);
				inProcessPages = new InProcessPagesDB(env);
				long numPreviouslyInProcessPages = inProcessPages.getLength();
				if (numPreviouslyInProcessPages > 0) {
					logger.info("Rescheduling " + numPreviouslyInProcessPages + " URLs from previous crawl.");
					scheduledPages -= numPreviouslyInProcessPages;
					while (true) {
						List<WebURL> urls = inProcessPages.get(100);
						if (urls.size() == 0) {
                            break;
                        }
						scheduleAll(urls);
						inProcessPages.delete(urls.size());
					}
				}
			} else {
				inProcessPages = null;
				scheduledPages = 0;
			}
		} catch (DatabaseException e) {
			logger.error("Error while initializing the Frontier: " + e.getMessage());
			workQueues = null;
		}
	}

	public void scheduleAll(List<WebURL> urls) {
		int maxPagesToFetch = config.getMaxPagesToFetch();
		synchronized (mutex) {
			int newScheduledPage = 0;
			for (WebURL url : urls) {
				if (maxPagesToFetch > 0 && (scheduledPages + newScheduledPage) >= maxPagesToFetch) {
					break;
				}
				try {
					workQueues.put(url);
					newScheduledPage++;
				} catch (DatabaseException e) {
					logger.error("Error while puting the url in the work queue.");
				}
			}
			if (newScheduledPage > 0) {
				scheduledPages += newScheduledPage;
				counters.increment(Counters.ReservedCounterNames.SCHEDULED_PAGES, newScheduledPage);
			}
			synchronized (waitingList) {
				waitingList.notifyAll();
			}
		}
	}

	public void schedule(WebURL url) {
		int maxPagesToFetch = config.getMaxPagesToFetch();
		synchronized (mutex) {
			try {
				if (maxPagesToFetch < 0 || scheduledPages < maxPagesToFetch) {
					workQueues.put(url);
					scheduledPages++;
					counters.increment(Counters.ReservedCounterNames.SCHEDULED_PAGES);
				}
			} catch (DatabaseException e) {
				logger.error("Error while puting the url in the work queue.");
			}
		}
	}

	public void getNextURLs(int max, List<WebURL> result) {
		while (true) {
			synchronized (mutex) {
				if (isFinished) {
					return;
				}
				try {
					List<WebURL> curResults = workQueues.get(max);//从队列中取出max个URL
					workQueues.delete(curResults.size());
					if (inProcessPages != null) {
						for (WebURL curPage : curResults) {
							inProcessPages.put(curPage);
						}
					}
					result.addAll(curResults);
				} catch (DatabaseException e) {
					logger.error("Error while getting next urls: " + e.getMessage());
					e.printStackTrace();
				}
				if (result.size() > 0) {
					return;
				}
			}
			try {
				synchronized (waitingList) {
					waitingList.wait();
				}
			} catch (InterruptedException ignored) {
			}
			if (isFinished) {
				return;
			}
		}
	}
	/**
	 * 抓取完成后将URL从inProcessPages中移除
	 * @param webURL
	 */
	public void setProcessed(WebURL webURL) {
		counters.increment(ReservedCounterNames.PROCESSED_PAGES);
		if (inProcessPages != null) {
			if (!inProcessPages.removeURL(webURL)) {
				logger.warn("Could not remove: " + webURL.getURL() + " from list of processed pages.");
			}
		}
	}
{% endhighlight %}



