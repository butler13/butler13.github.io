---
layout: post
title: 细说垂直型网络爬虫（十九）【突破访问限制】
category: 细说垂直型网络爬虫
date: 2014-12-29

---

<!-- more -->

##细说垂直型网络爬虫（十九）【突破访问限制】

标签： 网络爬虫 反爬虫

网络爬虫技术交流，请加QQ群：235971260

##网站的反爬虫
网络爬虫在访问网站并不是一个常规的用户行为，通常都会对网站的带宽和服务器带来一些压力。一些比较健壮的网络爬虫通常会控制对网站的访问频率。
但是大多数网络爬虫可能并不会采取措施，为了防止网络爬虫带来的负面影响，一些的网站会对来访者的访问做限制。

##常见的反爬虫措施
网站常用的反爬虫措施有：
1.人工识别、限制IP
当网站运维人员发现网站服务器负载突然增高等异常时，通过分析访问记录可以找到爬虫的来源IP，然后直接封锁爬虫的IP地址或者所在的C网段地址

**网络爬虫的应对措施：采用代理**

2.通过User-Agent+IP反爬虫
有时候通过简单的封锁C网段地址很可能造成“误伤”，一个C段网址中如果只有一个爬虫，那么其他的用户就被“误杀”了。不同的用户访问网站的时候有不同
的User-Agent，所以通过User-Agent+IP反爬虫也是一种策略。
爬虫在爬取网页的时候，为了尽可能的将自己伪装为浏览器，通常会声明自己的User-Agent信息，通过获取User-Agent信息和IP，网站就可以通过User-Agent+IP
的形式来反爬虫。

**网络爬虫的应对措施：采用代理+动态User-Agent**

3.延时响应
对于高级一点的爬虫，不管是限制IP还是User-Agent+IP反爬虫，都能轻松绕过。于是聪明的网站运维人员想出了一个高招，针对网络爬虫想要在短时间
内获取大量信息的特点，设计出了延时响应的反爬虫策略：如果网站反爬虫程序通过分析User-Agent和IP发现来访者可能是网络爬虫，则会对请求作出延时响应，如果延时的时间
超过了爬虫设置的连接超时时间，爬虫将报连接超时的错误。这种策略比User-Agent+IP好的地方就是可以减少一些“误伤”，但对于爬虫来说这种策略和上一种没有本质区别，
只是惩罚爬虫的方式不一样了。

**网络爬虫的应对措施：采用代理+动态User-Agent**

4.将重要数据使用JS加载
网络爬虫访问网站是为了采集需要的信息。如果将这些数据使用JS来加载，对于没有执行JS能力的大部分爬虫来说这简直就是灭顶之灾。
（各大电商网站就是使用这种方法）

**网络爬虫的应对措施：分析JS请求的URL以及参数，直接对此URL进行请求**

5.限制访问频率
对同一个IP或者User-Agent的访问频率做限制。如果爬虫的访问频率超过限制，则返回验证码页面或者限制访问。

**网络爬虫的应对措施：分布式或者适当减低访问频率**

6.需要登录访问
如果是内容比较优质的网站，通常会要求用户登录后访问。而对于爬虫来说，登录是一个难点，尤其是有复杂验证码的登陆。

**网络爬虫的应对措施：程序识别验证码模拟登陆、人工识别验证码模拟登陆**

爬虫和反爬虫的终于对决（假设一下）：
反爬虫：如上六种反爬策略随意组合，此外设置爬虫陷阱。
爬虫：无节操的高并发访问网站，玉石俱焚。

题外话：
对于网站来说，反爬虫措施过于严格可能导致百度谷歌等搜索引擎不能收录网站。
对于爬虫开发者来说，在抓取数据的时候尽量不要对目标网站造成太多的负面影响。


