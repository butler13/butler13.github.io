---
layout: post
title: 细说垂直型网络爬虫（十四）【源码解读之Crawl4J（1）】
category: 细说垂直型网络爬虫
date: 2014-12-9

---

##细说垂直型网络爬虫（十四）【源码解读之Crawl4J（1）】

标签： 网络爬虫 开源爬虫源码解读 Crawl4J

###Crawl4J简介
Crawl4J是一个开源的Java爬虫程序，总共才三十多个类，比较简单，非常适合爬虫入门的学习。

<!-- more -->

Crawl4J的主要包有：
>
edu.uci.ics.crawler4j.crawler 基本抓取逻辑和抓取配置
edu.uci.ics.crawler4j.fetcher 爬取（从网络上获取页面）
edu.uci.ics.crawler4j.frontier 抓取的URL队列相关
edu.uci.ics.crawler4j.parser 解析（解析获取到的页面，对数据进行结构化）
edu.uci.ics.crawler4j.robotstxt robots.txt协议相关
edu.uci.ics.crawler4j.url URL相关（WebURL）
edu.uci.ics.crawler4j.util 工具类

![Crawl4J主要的包](/res/img/blogimg/2014121102.jpg)

###利用Crawl4J抓取一个网站
继承WebCrawler,重写shouldVisit和visit方法：
{% highlight java %}
public class MyCrawler extends WebCrawler{
	private final static Pattern FILTERS = Pattern.compile(".*(\\.(css|js|bmp|gif|jpe?g"
            + "|png|tiff?|mid|mp2|mp3|mp4"
            + "|wav|avi|mov|mpeg|ram|m4v|pdf"
            + "|rm|smil|wmv|swf|wma|zip|rar|gz))$");

	@Override
	public void onStart() {
		super.onStart();
	}

	/**
	 * 过滤URL
	 */
	@Override
	public boolean shouldVisit(WebURL url) {
		String href = url.getURL().toLowerCase();
        return !FILTERS.matcher(href).matches() ;
	}

	/**
	 * 处理获取到的页面
	 */
	@Override
	public void visit(Page page) {
		String url = page.getWebURL().getURL();
        if (page.getParseData() instanceof HtmlParseData) {
                HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();
                String text = htmlParseData.getText();
                String html = htmlParseData.getHtml();
                List<WebURL> links = htmlParseData.getOutgoingUrls();
                String title=htmlParseData.getTitle();

                System.out.println("Title:"+title);
                System.out.println("Text length: " + text.length());
                System.out.println("Html length: " + html.length());
                System.out.println("Number of outgoing links: " + links.size());
        }
	}
}
{% endhighlight %}

使用CrawlController启动爬虫：
{% highlight java %}
public static void main(String[] args) {
		String crawlStorageFolder = "D:\\Crawler\\data";
		int numberOfCrawlers = 7;
		CrawlConfig config = new CrawlConfig();
		config.setCrawlStorageFolder(crawlStorageFolder);
		PageFetcher pageFetcher = new PageFetcher(config);
		RobotstxtConfig robotstxtConfig = new RobotstxtConfig();
		RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);
		try {
			CrawlController controller = new CrawlController(config, pageFetcher, robotstxtServer);
			controller.addSeed("http://www.jiajmh.com/company.htm");
			controller.start(MyCrawler.class, numberOfCrawlers);
		} catch (Exception e) {
			e.printStackTrace();
		}
	}
{% endhighlight %}

















